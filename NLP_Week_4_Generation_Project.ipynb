{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbooMardiiyah/text-generation-with-gpt-2-3/blob/main/NLP_Week_4_Generation_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfs1-EmH7ufz"
      },
      "source": [
        "# Week 4: Text Generation\n",
        "\n",
        "### What we are building\n",
        "A smart compose system that assists in writing movie reviews using the IMDB movie review dataset. FYI: You probably interact with smart compose multiple times a day while typing in Gmail, typing on your phone, or just using Google search.\n",
        "\n",
        "### Instructions\n",
        "\n",
        "We will compare a really simple memorization model that just remembers how often certain words follow a phrase with a pre-trained GPT-2. Finetuning a GPT-2 can take a long time even with a GPU so we'll leave that as an extension project.\n",
        "\n",
        "### Code Overview\n",
        "\n",
        "- Dependencies: Install and import python dependencies\n",
        "- Datasets - Methods and dataset for evaluation\n",
        "- Models\n",
        "  - Memorization\n",
        "  - GPT-2 Pretrained\n",
        "- Extensions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8zLCEfd7VKI"
      },
      "source": [
        "# Dependencies\n",
        "\n",
        "âœ¨ Now let's get started! To kick things off, as always, we will install some dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdNdHsyAKXzZ"
      },
      "source": [
        "%%capture\n",
        "# Install all the required dependencies for the project\n",
        "!pip install transformers==4.17.0\n",
        "!pip install datasets==1.15.1\n",
        "!pip install pytorch-lightning==1.6.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BccBURgfO_3t"
      },
      "source": [
        "Import all the necessary libraries we need throughout the project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4BCnkSaphtI"
      },
      "source": [
        "# Import all the relevant libraries\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2TokenizerFast\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from datasets import load_dataset_builder\n",
        "from datasets import load_dataset\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import torchmetrics\n",
        "import pytorch_lightning as pl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy9Pc5zAv1KG"
      },
      "source": [
        "### Dataset Loading (common to all solutions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml67kfADUyHr"
      },
      "source": [
        "dataset_builder = load_dataset_builder('imdb')\n",
        "train_dataset = [d[\"text\"] for d in load_dataset('imdb', split='train')]\n",
        "test_dataset = [d[\"text\"] for d in load_dataset('imdb', split='test')]\n",
        "\n",
        "print(f\"Length of training data: {len(train_dataset)}\")\n",
        "print(f\"Length of test data: {len(test_dataset)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vZNSpB6PVHe"
      },
      "source": [
        "### Evaluation Dataset\n",
        "\n",
        "Running GPT-2 is really expensive so we create a small sample dataset of size 500 and use that for our evaluations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqGEabKMv4yZ"
      },
      "source": [
        "# Fix the random seed\n",
        "np.random.seed(0)\n",
        "\n",
        "def create_eval_dataset(dataset, num_examples=500):\n",
        "  if len(dataset) < num_examples:\n",
        "    raise ValueError(f\"Can not select {num_examples} unique examples from dataset of size {len(dataset)}\")\n",
        "\n",
        "  # Since it is really expensive to run GPT, we'll use a smaller dataset for eval\n",
        "  sample = np.random.choice(dataset, num_examples, replace=False)\n",
        "\n",
        "  prefixes = []\n",
        "  output_words = []\n",
        "  for d in sample:\n",
        "    words = d.lower().split(\" \")\n",
        "    boundary = np.random.randint(1, len(words)-1)\n",
        "    prefix = \" \".join(words[:boundary])\n",
        "    prefixes.append(prefix)\n",
        "    output_words.append(words[boundary])\n",
        "  return prefixes, output_words\n",
        "\n",
        "prefixes, output_words = create_eval_dataset(test_dataset, 500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC5yAi2oPsUl"
      },
      "source": [
        "**Evaluation Function**: Create a single function to compute correct predictions in the top_k from the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csTmZ4MNFLsQ"
      },
      "source": [
        "def evaluate_exact_match_at(model, prefixes, output_words, top_k):\n",
        "  em_count = 0\n",
        "  i = 0\n",
        "  for i, (prefix, output_word) in enumerate(zip(prefixes, output_words)):\n",
        "    for p in model.predict(prefix, top_k):\n",
        "      if p.strip() == output_word.strip():\n",
        "        em_count += 1\n",
        "        break\n",
        "    if i % 20 == 0:\n",
        "      print(f\"Evaluated {i} prefixes\")\n",
        "  print(f\"Exact match evaluation em@{top_k}:{em_count /len(prefixes)} . Model got {em_count} matches out of {len(prefixes)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9idYvkY37zhJ"
      },
      "source": [
        "# Memorizer\n",
        "\n",
        "Model takes the largest prefix it will memorize which defaults to 3. This means for each sentence of the 4 words such as `I like learning NLP` it'll memorize that it saw `NLP` follow the prefix `I like learning` once.  \n",
        "\n",
        "The model also memorizes any window of size between 1 to the largest_prefix length that are fall back options if we encounter new words. So following our example the model has learned the following:\n",
        "\n",
        "```python\n",
        "[\n",
        "  ('I like learning', 'NLP'),\n",
        "  ('I like', 'learning'), ('like learning', 'NLP'),\n",
        "  ('I', 'like'), ('like', 'learning'), ('learning', 'NLP'),\n",
        "]\n",
        "```\n",
        "\n",
        "This is done so that if we encounter a sentence like `We like learning` we can fall back to the prefix of length 2 and then predict `NLP`.\n",
        "\n",
        "**Implement** the predict function that checks from the largest to the smaller possible prefix and uses the memory dictionary to make predictions and returns the top_k."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4gLdAfbTLdv"
      },
      "source": [
        "## ASSIGNMENT PART 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd9kXfokbZzn"
      },
      "source": [
        "def _window(seq, n=2):\n",
        "  \"\"\"Returns a sliding window based on n\n",
        "  \"\"\"\n",
        "  seq = tuple(seq)\n",
        "  if len(seq) < n: \n",
        "    return []\n",
        "  for i in range(0, len(seq) - n + 1):\n",
        "    yield seq[i:i+n]\n",
        "\n",
        "\n",
        "class Memorizer:\n",
        "  def __init__(self, train_dataset, largest_prefix=3): \n",
        "    self.largest_prefix = largest_prefix\n",
        "    self.memory = {}\n",
        "    # Build the dictionaries for each prefix length\n",
        "    for prefix_size in range(largest_prefix+1):\n",
        "      self.memory[prefix_size] = defaultdict(Counter)\n",
        "      self._build(train_dataset, prefix_size + 1, self.memory[prefix_size])\n",
        "\n",
        "  def _build(self, train_dataset, window_size, memory):\n",
        "    \"\"\"Build the memory dictionary for a provided window_size\n",
        "    \"\"\"\n",
        "    for data in train_dataset:\n",
        "      words = data.split(\" \")\n",
        "      # Compute the different word windows using the _window function\n",
        "      for window in _window(words, window_size):\n",
        "        if window_size == 1:\n",
        "          # There is no window, just memorize how frequently each word occurs in the dataset\n",
        "          output_word = window[0]\n",
        "          # Default all the prefixes to UNK\n",
        "          prefix = \"UNK\"\n",
        "        else:\n",
        "          # Use the prefix and update the count of the word that follows it\n",
        "          prefix = \" \".join(window[:-1])\n",
        "          output_word = window[-1]\n",
        "        memory[prefix][output_word] += 1\n",
        "\n",
        "  def predict(self, prefix, top_k=1):\n",
        "    \"\"\"Top_k words that might follow the given the prefix in our dataset\n",
        "    \"\"\"\n",
        "    prefix_words = prefix.split(\" \")\n",
        "    for prefix_len in range(min(len(prefix_words), self.largest_prefix), 0, -1):\n",
        "\n",
        "      # Compute the prefix string for the size of the window\n",
        "      ### TO BE COMPLETED ### \n",
        "      prefix_str = ...\n",
        "      ### TO BE COMPLETED ### \n",
        "\n",
        "      # If prefix is in memory \"return\" the top_k matches \n",
        "      # Remember we've to return here since we want to use the data from the longest prefix that matches\n",
        "      if prefix_str in self.memory[prefix_len]:\n",
        "        ### TO BE COMPLETED ### \n",
        "        return ...\n",
        "        ### TO BE COMPLETED ###\n",
        "\n",
        "    # None of the prefix matched so just return the most common words in the dataset\n",
        "    predictions = self.memory[0][\"UNK\"].most_common(top_k)\n",
        "    return [p[0] for p in predictions]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VazPFsZ1Ti5p"
      },
      "source": [
        "## Experiment with Memorizer widget\n",
        "\n",
        "Ha! Here is a cute trick to build fun widgets within the colab. Just try different sentences for the dataset and prefix to see if the memorizer is working correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOhihw5x8oRU"
      },
      "source": [
        "#@title Experiment with Memorizer\n",
        "\"\"\"\n",
        "In this cell, we've built a toy dataset with only 3 examples. \n",
        "Now given the prefix 'I like', a trie would emit 'football' and 'tennis' based \n",
        "on the co-occurence.\n",
        "\"\"\"\n",
        "dataset_1 = \"I like football\" #@param {type:\"string\"}\n",
        "dataset_2 = \"I like tennis sometimes\" #@param {type:\"string\"}\n",
        "dataset_3 = \"I like football way too much\" #@param {type:\"string\"}\n",
        "prefix = \"I like\" #@param {type:\"string\"}\n",
        "\n",
        "memorized_toy_model = Memorizer([dataset_1, dataset_2, dataset_3])\n",
        "\n",
        "predictions = memorized_toy_model.predict(prefix, 2)\n",
        "\n",
        "# The model should predict [football, tennis]\n",
        "# since football occurred twice while tennis was just once.\n",
        "print(\"Predictions: \", predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr0JHmxsT79f"
      },
      "source": [
        "### Train memorizer on the actual training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6V8N3peEbhd"
      },
      "source": [
        "memorized_model = Memorizer(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No-kJxjxUABN"
      },
      "source": [
        "### Evaluation on top_1 and top_3\n",
        "##### <font color='red'>Expected em@1: ~0.12%</font>\n",
        "##### <font color='red'>Expected em@3: ~0.122%</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0azpnN_aF9Vl"
      },
      "source": [
        "evaluate_exact_match_at(memorized_model, prefixes, output_words, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml6fiVhaCOhC"
      },
      "source": [
        "evaluate_exact_match_at(memorized_model, prefixes, output_words, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omWKcpYxHRpT"
      },
      "source": [
        "## GPT-2: Generative Pre-trained Transformer\n",
        "\n",
        "We'll use the pretrainined GPT-2 model provided by the transformers package. Make sure you implement the predict function.\n",
        "\n",
        "Implementation Steps:\n",
        "1. Encode the sentence using `tokenizer.encode` and make sure it returns a torch tensor.\n",
        "2. Run this through the model and those are your predictions.\n",
        "3. Decode the indices from the output of top_k using the tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1VOee59YGU1"
      },
      "source": [
        "### ASSIGNMENT PART 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0WYO_MpXoDl"
      },
      "source": [
        "class GPT2PreTrained:\n",
        "  def __init__(self): \n",
        "    self.tokenizer = GPT2TokenizerFast.from_pretrained('distilgpt2')\n",
        "    self.model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "    self.model.eval()\n",
        "\n",
        "  def predict(self, prefix, top_k=1):\n",
        "    ### TO BE IMPLEMENTED ### \n",
        "    indexed_tokens = ...\n",
        "    predictions = ...\n",
        "    ### TO BE IMPLEMENTED ### \n",
        "\n",
        "    _, indices = torch.topk(predictions[0][0, -1, :], k=top_k)\n",
        "    ### TO BE IMPLEMENTED ### \n",
        "    predictions = ...\n",
        "    ### TO BE IMPLEMENTED ### \n",
        "    \n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foxA5kgTXrjN"
      },
      "source": [
        "### Experiment with GPT-2 Widget"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MQXTUvPDp2k"
      },
      "source": [
        "#@title Experiment with GPT-2\n",
        "\"\"\"\n",
        "In this cell, we've built a toy prompt from which we predict \n",
        "the next words using GPT-2.\n",
        "\"\"\"\n",
        "text = \"pitcher threw a\" #@param {type:\"string\"}\n",
        "\n",
        "gpt_model = GPT2PreTrained()\n",
        "\n",
        "predictions = gpt_model.predict(text, 2)\n",
        "## Output should be \"pitch, ball\" or something similar\n",
        "print(\"Predictions: \", predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y4sqtI2YA7M"
      },
      "source": [
        "### Evaluation on top_1 and top_3\n",
        "##### <font color='red'>Expected em@1: ~0.21%</font>\n",
        "##### <font color='red'>Expected em@3: ~0.298%</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb1gU6S5IyXW"
      },
      "source": [
        "evaluate_exact_match_at(gpt_model, prefixes, output_words, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88s8LvLgA3le"
      },
      "source": [
        "evaluate_exact_match_at(gpt_model, prefixes, output_words, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLtHM8gyavwB"
      },
      "source": [
        "ðŸŽ‰ YAYYYY!!! We did it, that's it. Take a second to pause how many different things you've tried in the last 4 weeks. Go you!!\n",
        "\n",
        "## Extensions\n",
        "- Build an LSTM based generation model (Remember to cut sequences at about 10-15 words, LSTMs don't work on long sentences).\n",
        "- Try fine-tuning the GPT-2 model using a GPU runtime for the notebook. (NOTE: colab free GPUs are pretty bad so this is probably not worth doing in the free tier)"
      ]
    }
  ]
}